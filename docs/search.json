[
  {
    "objectID": "course_notes.html",
    "href": "course_notes.html",
    "title": "Course Notes",
    "section": "",
    "text": "Here are the course notes written by myself. I’ve taken 58 courses (147 points) in PKU.\n\n24-25-3\n2 courses, 4 points in total.\n\nTopics in Quantitative Finance | Click here to view the website.  量化金融专题 | 点击此处访问网站.\nInter-cultural Communication  跨文化交流学\n\n\n\n24-25-2\n8 courses, 25 points in total.\n\nIntroduction to Database Systems | Click here to view the PDF.  数据库概论 | 点击此处查看PDF.\nPublic Intermediate German  公共中级德语\n\nThe notes. 课程笔记.\nThe tests. 课程小测.\nThe words. 课程单词.\nHere are two passages full of grammer mistakes: Ein Programmierer and Meine Ferien.\n\nSpecial Topics in China Economic Research | Click here to view the PDF.  中国经济专题 | 点击此处查看PDF.\nIntermediate Microeconomics  中级微观经济学\nProgramming in Rust  Rust程序设计\nOrienteering and Hiking  定向与徒步运动\nOperating Systems (Honor Track)  操作系统 (实验班)\nModern Astronomy  现代天文学\n\n\n\n24-25-1\n6 courses, 21 points in total.\n\nPublic Junior German | Click here to view the PDF.  公共初级德语 | 点击此处查看PDF.\nEconometrics | Click here to view the PDF.  计量经济学 | 点击此处查看PDF.\nSoftware Engineering  软件工程\n\nThe notes. 课程笔记.\nThe Assignments. 课程作业.\n\nHomework 1\nHomework 2\nHomework 3\n\n\nA Survey of Mao Tsetung Thoughts and Theory of Socialism with Chinese Charateristics  毛概\nSoftware Analysis  软件分析技术\nCompiler Principles  编译原理\n…\n\n\n\n23-24-3\n2 courses, 3 points in total.\n\nPhysical and Aesthetic course  体美\nAural Culture and World Civilization  听觉文化与世界文明\n\n\n\n23-24-2\n9 courses, 24 points in total.\n\nIntroduction to Object Oriented Technology  面向对象技术引论\nComparative Constitutional Law  外国宪法\nAcademic English Listening and Speaking  学术英语听说\nAlgorithm Design and Anslysis  算法设计与分析\nInformation Theory  信息论\nHistory of Rome  罗马史\nSoftware Foundations  软件科学基础\nProbability theory and statistics in Information science  信息学中的概率统计\nIntermediate Macroeconomics  中级宏观经济学\n\n\n\n23-24-1\n8 courses, 24 points in total.\n\nHuman Sex, Reproduction and Health  人类的性、生育与健康\nMusic and Mathematics  音乐与数学\nData Structure and Algorithm (A)  数据结构与算法 (A)\nAn Introduction to Marxist Basic Theory  马原\nIntroduction to Discrete Mathmatics  离散数学基础\nPrinciples of Economics  经济学原理\nIntroduction to Computer Systems  计算机系统导论\n…\n\n\n\n22-23-2\n8 courses, 24 points in total.\n\nText and Meaning: British and American Short Stories  英美短篇小说文本分析与鉴赏\nIntroduction to Artificial Intelligence  人工智能引论\nPractice of Programming in C&C++  程序设计实习\nOutline of Chinese Modern History  中国近现代史纲要\n习概\nLinear Algebra A (II)  线性代数A (II)\nAdvanced Mathematics A (no.2)  高等数学A (二)\n…\n\n\n\n22-23-1\n11 courses, 27 points in total.\n\nIntroduction to Seismology  地震概论\nIntroduction to Computation (A)  计算概论A\nAdvanced Mathematics A (no.1)  高等数学A (一)\nFootball  足球\nPhysics for Information Sciences (1)  信息科学中的物理学 (上)\nLinear Algebra A (I)  线性代数A (I)\nIntroduction to Information Science and Technique  信息科学技术概论\n…\n\nTO BE CONTINUED …"
  },
  {
    "objectID": "Paper_Notes/FSE20.html",
    "href": "Paper_Notes/FSE20.html",
    "title": "Detecting numerical bugs in neural network architectures",
    "section": "",
    "text": "This paper makes the first attempt to conduct static analysis for detecting numerical bugs at the architecture level."
  },
  {
    "objectID": "Paper_Notes/FSE20.html#introduction",
    "href": "Paper_Notes/FSE20.html#introduction",
    "title": "Detecting numerical bugs in neural network architectures",
    "section": "Introduction",
    "text": "Introduction\nA neural architecture can contain numerical bugs that cause serious consequences. Numerical bugs in a neural architecture manifest themselves as numerical errors in the form of “NaN”, “INF”, or crashes during training or inference.\n# Input:\n# center: 2*100 - shape tensor whose elements in [-1, 1]\n# offset : 2*100 - shape tensor whose elements in [0, 2]\n\n# Create 100 rectangles .\nbottomLeft = center - offset\ntopRight = center + offset\nrectangle = tf.concat([bottomLeft, topRight], axis=1)\n\n# Calculate the reciprocal of their areas.\nbottom, left, top, right = tf.split(rectangle, num_or_size_splits=4, axis=1)\nwidth = right - left\nheight = top - bottom\narea = width * height\nscale = tf.reciprocal(area)\nThe key insight is that affine relations are common in neural network architectures.\n\nBasics of Abstract Interpretation\nConcrete properties are described in the concrete domain \\(\\mathbb{C}\\) with a partial order \\(\\subseteq\\), and abstract properties are described in the abstract domain \\(\\mathbb{A}\\) with a partial order \\(\\sqsubseteq\\).\n\nAbstraction function \\(\\alpha: \\mathbb{C} \\rightarrow \\mathbb{A}\\)\nConcretization function \\(\\gamma: \\mathbb{A} \\rightarrow \\mathbb{C}\\)\n\nGalois connection \\(\\langle\\mathbb{C},\\subseteq\\rangle \\overset{\\gamma}{\\underset{\\alpha}{\\leftrightarrows}} \\langle\\mathbb{A},\\sqsubseteq\\rangle\\): \\[\n\\forall c \\in \\mathbb{C}, a \\in \\mathbb{A}. \\alpha(c) \\sqsubseteq a \\Leftrightarrow c \\subseteq \\gamma(a)\n\\]\n\n\nAbstract Domain of Intervals\n\\[\n\\mathbb{A}_I \\triangleq \\{([l_1,u_1],...,[l_n,u_n] | l,u \\in \\mathbb{R}^n)\\}\n\\] Here \\(x \\in \\mathbb{A}_I \\Rightarrow l_i \\leq x_i \\leq u_i\\).\nAlso, we say \\(a_1 \\sqsubseteq a_2\\) iff \\(\\forall i. [l_i^1,u_i^1] \\subseteq [l_i^2,u_i^2]\\).\nAnd the abstraction functions are defined as: \\[\n\\alpha_I(c) = ([\\inf\\{x_i | x \\in c\\}, \\sup\\{x_i | x \\in c\\}])_{i=1}^n\n\\] That is, the abstraction function maps a concrete set to the smallest interval containing it.\nThe concretization function is defined as: \\[\n\\gamma_I(a) = \\{x \\in \\mathbb{R}^n | \\forall i. l_i^a \\leq x_i \\leq u_i^a\\}\n\\]\n\n\nAbstract Domain of Affine Relations\n\\[\n\\mathbb{A}_E \\triangleq \\{(\\mathbf{A},b)|\\mathbb{A} \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m, m&gt;0\\}\n\\]\nHere \\(x \\in \\mathbb{A}_E \\Rightarrow \\mathbf{A}x = b\\).\nAnd the abstraction functions are defined as:\n\\[\n\\alpha_E(c) = \\begin{cases}\n(\\mathbf{A},b) & \\text{if } c \\subseteq \\{x | \\mathbf{A}x = b\\} \\text{ and } (\\mathbf{A},b) \\text{ is in reduced echelon form}\\\\\n\\top & \\text{if } c = \\mathbb{R}^n\\\\\n\\bot & \\text{otherwise}\n\\end{cases}\n\\]\nThe concretization function is defined as:\n\\[\n\\gamma_E(a) = \\{x \\in \\mathbb{R}^n | \\mathbf{A}x = b\\},\n\\] here \\(a = (\\mathbf{A},b)\\).\n\n\nAbstract Domain for Neural Architectures\nThe abstract domain for Tensor partitioning and Interval abstraction with affine Equality relation \\(\\mathbb{A}_{\\text{TIE}}\\) is defined as: \\[\n\\mathbb{A}_{\\text{TIE}} \\triangleq \\{(\\mathcal{P}, a^{\\sharp I}, a^{\\sharp E})|a^{\\sharp I} \\in \\mathbb{A}_I, a^{\\sharp E}\\in\\mathbb{A}_E\\},\n\\] where \\(\\mathcal{P} = \\{A_1,A_2,...,A_n\\}\\) is a tensor partitioning.\nThe concretization function \\(\\gamma_{\\text{TIE}}\\) of an element \\(a^\\sharp = (\\mathcal{P}, a^{\\sharp I}, a^{\\sharp E})\\) is defined as: \\[\n\\gamma_{\\text{TIE}}(a^\\sharp) = \\gamma_I(a^{\\sharp I}) \\cap \\gamma_E(a^{\\sharp E})\n\\]\nNotes for ReLU:\n\nCreate a new symbolic variable \\(b\\) for \\(a\\).\nAnd we have \\(b - a^{\\text{ReLU}} = 0\\).\n\\(a^{\\text{ReLU}} - a^{-\\text{ReLU}} - a =0\\)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, this is VectorPikachu, or Hangzhou Lyu (吕杭州, lǚ háng zhōu). Currently I’m a student majored in Software Engineering. And my research interest lies on Program Verification, including Dafny, Coq, Verus, et.al."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\n\nPeking University | Beijing, China  B.A. in Software Engineering | Sept 2022 - June 2026  北京大学 | 中国北京  软件工程 | 2022 年 9 月 - 2026 年 6 月"
  },
  {
    "objectID": "about.html#certificates",
    "href": "about.html#certificates",
    "title": "About",
    "section": "Certificates",
    "text": "Certificates\n\nCollege English Test Band4, CET4 | 671  大学英语四级考试 | 671\nCollege English Test Band6, CET6 | 650  大学英语六级考试 | 650"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "VectorPikachu’s Website",
    "section": "",
    "text": "Here is the homepage of VectorPikachu. You can navigate to the About page to learn more about me. You can also check out my Course Notes for various courses I’ve taken in PKU. Paper Notes are the notes of the paper I’ve read.\n\nAbout Me\nCourse Notes\nPaper Notes"
  },
  {
    "objectID": "paper_notes.html",
    "href": "paper_notes.html",
    "title": "Paper Notes",
    "section": "",
    "text": "Here are the notes of the papers I’ve read."
  },
  {
    "objectID": "paper_notes.html#formal-methods-in-ai",
    "href": "paper_notes.html#formal-methods-in-ai",
    "title": "Paper Notes",
    "section": "Formal Methods in AI",
    "text": "Formal Methods in AI\nStatic Analysis for DL Architectures:\n\nDEBAR: Detecting numerical bugs in neural network architectures (Zhang et al. 2020)\n\nAffine Relations Abstract Domain.\nTensor Partitioning.\n\nRANUM: Reliability Assurance for Deep Neural Network Architectures Against Numerical Defects (Li et al. 2023)\n\nPotential-Defect Detection via Static Analysis.\nFeasibility Confirmation via Two-Step Test Generation.\nFix Suggestion via Abstract Optimization.\n\n\nStatic Analysis for DL Models:\n\nUse SMT solvers to verify properties of DL models.\nUse integer programming to verify robustness of DL models.\nAbstract interpretation for DL models. (e.g., \\(\\text{AI}^2\\), DeepPoly, etc.)\n\nZonotope abstract domain.\n\n\nSOTA tools:\n\n\\(\\alpha,\\beta\\)-CROWN"
  },
  {
    "objectID": "Paper_Notes/ICSE23.html",
    "href": "Paper_Notes/ICSE23.html",
    "title": "Reliability Assurance for Deep Neural Network Architectures Against Numerical Defects",
    "section": "",
    "text": "To assure high reliability against numerical defects, in this paper, we propose the RANUM approach including novel techniques for three reliability assurance tasks: detection of potential numerical defects, confirmation of potential-defect feasibility, and suggestion of defect fixes. To the best of our knowledge, RANUM is the first approach that confirms potential-defect feasibility with failure-exhibiting tests and suggests fixes automatically.\nBut here I will only focus on the detection of potential numerical defects via static analysis."
  },
  {
    "objectID": "Paper_Notes/ICSE23.html#static-analysis-for-potential-defect-detection",
    "href": "Paper_Notes/ICSE23.html#static-analysis-for-potential-defect-detection",
    "title": "Reliability Assurance for Deep Neural Network Architectures Against Numerical Defects",
    "section": "Static Analysis for Potential-Defect Detection",
    "text": "Static Analysis for Potential-Defect Detection\nThe computational graph can be viewed as a Directed Acyclic Graph (DAG): \\(\\mathcal{G} = \\langle\\mathcal{V}, \\mathcal{E} \\rangle\\).\nWe call nodes with zero in-degree as initial nodes, which correspond to input, weight, or constant nodes.\nWe call nodes with positive in-degree as internal nodes, which correspond to concrete operators, such as matrix multiplication (MatMul) and addition (Add).\nWe let \\(\\mathbfit{x}\\) and \\(\\mathbfit{w}\\) denote the concatenation of data from all input nodes and data from all weight nodes, respectively. E.g., in @#fig-ranum-computational-graph, \\(\\mathbfit{x}\\) concatenates data from nodes 1 and 11; and \\(\\mathbfit{w}\\) concatenates data from nodes 2 and 4.\nWe use \\(f_n^{\\textsf{in}}(\\mathbfit{x}; \\mathbfit{w})\\) and \\(f_n^{\\textsf{out}}(\\mathbfit{x}; \\mathbfit{w})\\) to express input and output data of node \\(n\\), respectively, given \\(\\mathbfit{x}\\) and \\(\\mathbfit{w}\\).\ninput_data = tf.placeholder(\"float\", [1, n_features], name='x-input')\ninput_labels = tf.placeholder(\"float\", [1, n_classes], name='y-input')\nself.W_ = tf.Variable(tf.zeros([n_features, n_classes]), name='weights')\nself.b_ = tf.Variable(tf.zeros([n_classes]), name='biases')\nmodel_output = tf.nn.softmax(tf.matmul(input_data, self.W_) + self.b_)\ncost = -tf.reduce_mean(input_labels * tf.log(model_output) +\n                       (1 - input_labels) * tf.log(1 - model_output),\n                       name='cost')\nself.obj_function = tf.reduce_min(tf.abs(model_output), name='obj_function')\n\n\n\n\n\n\nFigure 1: Computational graph encoded by the snippet\n\n\n\nDefinition. For the given computational graph \\(\\mathcal{G} = \\langle\\mathcal{V}, \\mathcal{E} \\rangle\\), if there is a node \\(n_0 \\in \\mathcal{V}\\), such that there exists a valid input and valid weights that can let the input of node \\(n_0\\) fall within the invalid range, we say there is a numerical defect at node \\(n_0\\). Formally, \\(\\exists \\mathbfit{x}_0 \\in \\mathcal{X}_{\\textsf{valid}}, \\mathbfit{w}_0 \\in \\mathcal{W}_{\\textsf{valid}}, f_{n_0}^{\\textsf{in}}(\\mathbfit{x}_0; \\mathbfit{w}_0) \\in \\mathcal{I}_{n_0,\\textsf{invalid}} \\Longrightarrow\\) \\(\\exists\\) numerical defect at node \\(n_0\\).\nFor example, ImageNet Resnet50 models have - valid input range \\(\\mathcal{X}_{\\textsf{valid}} = [0,1]^{3\\times 224 \\times 224}\\): image pixel intensities are within \\([0,1]\\); - valid weight range \\(\\mathcal{W}_{\\textsf{valid}} = [-1,1]^{\\textsf{weight_shape}}\\): weights of well-trained Resnet50 models are typically within \\([-1,1]\\); - The invalid range \\(\\mathcal{I}_{n_0,\\textsf{invalid}}\\): determined by the operator type of node \\(n_0\\). For example, for the Log operator, the invalid range is \\((-\\infty,U_{\\text{min}}]\\), where \\(U_{\\text{min}}\\) is the smallest positive number of a tensor’s data type.\n\nDNN Static Analysis Framework with Backward Fine-Grained Node Labeling for Potential-Defect Detection\nA DNN architecture + valid ranges for input and weight nodes \\(=&gt;\\) interval abstractions for possible inputs and outputs of each node.\nFormally, for given valid ranges of inference input and model weights, namely \\(\\mathcal{X}\\) and \\(\\mathcal{W}\\), for each node \\(n \\in \\mathcal{V}\\), our framework computes sound input interval abstraction \\([\\mathbfit{l}_n, \\mathbfit{u}_n] := \\{\\mathbfit{x} : \\mathbfit{l}_n \\leq x \\leq \\mathbfit{u}_n\\}\\) such that \\([\\mathbfit{l}_n,\\mathbfit{u}_n]\\) always captures all possible inputs of the node: \\([\\mathbfit{l}_n,\\mathbfit{u}_n] \\supseteq \\{f_n^{\\textsf{in}}(\\mathbfit{x}, \\mathbfit{w}) : \\mathbfit{x} \\in \\mathcal{X} , \\mathbfit{w} \\in \\mathcal{W}\\}\\). We also compute output interval abstractions similarly.\nThe interval domain with tensor partitioning provides a degree of freedom in terms of the partition granularity, i.e., we can choose the subblock size for each node’s abstraction. When the finest granularity, i.e., elementwise abstraction, is chosen, the abstraction interval is the most concrete. When the coarsest granularity (i.e., one scalar to summarize the node tensor) is chosen, the abstraction saves the most space and computational cost but loses much precision.\nE.g., \\(([−1, 0], [0, 1], [1, 2], [−1, 0])\\): - the finest granularity, \\([\\mathbfit{l}_n, \\mathbfit{u}_n] = [(−1, 0, 1, −1),(0, 1, 2, 0)]\\). - the coarsest granularity, \\([\\mathbfit{l}_n, \\mathbfit{u}_n] = [−1, 2]\\).\nUsing the finest instead of the coarsest granularity for some nodes is more beneficial for overall abstraction preciseness. - the control-flow operators, e.g., Loop - the indexing operators, e.g., Slice - shaping operators, e.g., Reshape"
  }
]
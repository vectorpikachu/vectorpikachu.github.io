@inproceedings{FSE20Zhang,
  author    = {Zhang, Yuhao and Ren, Luyao and Chen, Liqian and Xiong, Yingfei and Cheung, Shing-Chi and Xie, Tao},
  title     = {Detecting numerical bugs in neural network architectures},
  year      = {2020},
  isbn      = {9781450370431},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3368089.3409720},
  doi       = {10.1145/3368089.3409720},
  abstract  = {Detecting bugs in deep learning software at the architecture level provides additional benefits that detecting bugs at the model level does not provide. This paper makes the first attempt to conduct static analysis for detecting numerical bugs at the architecture level. We propose a static analysis approach for detecting numerical bugs in neural architectures based on abstract interpretation. Our approach mainly comprises two kinds of abstraction techniques, i.e., one for tensors and one for numerical values. Moreover, to scale up while maintaining adequate detection precision, we propose two abstraction techniques: tensor partitioning and (elementwise) affine relation analysis to abstract tensors and numerical values, respectively. We realize the combination scheme of tensor partitioning and affine relation analysis (together with interval analysis) as DEBAR, and evaluate it on two datasets: neural architectures with known bugs (collected from existing studies) and real-world neural architectures. The evaluation results show that DEBAR outperforms other tensor and numerical abstraction techniques on accuracy without losing scalability. DEBAR successfully detects all known numerical bugs with no false positives within 1.7–2.3 seconds per architecture. On the real-world architectures, DEBAR reports 529 warnings within 2.6–135.4 seconds per architecture, where 299 warnings are true positives.},
  booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages     = {826–837},
  numpages  = {12},
  keywords  = {Static Analysis, Numerical Bugs, Neural Network},
  location  = {Virtual Event, USA},
  series    = {ESEC/FSE 2020}
}

@inproceedings{ICSE23Li,
  author    = {Li, Linyi and Zhang, Yuhao and Ren, Luyao and Xiong, Yingfei and Xie, Tao},
  booktitle = {2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
  title     = {Reliability Assurance for Deep Neural Network Architectures Against Numerical Defects},
  year      = {2023},
  volume    = {},
  number    = {},
  pages     = {1827-1839},
  keywords  = {Source coding;Artificial neural networks;Computer architecture;Benchmark testing;Reliability;Task analysis;Optimization;neural network;numerical defect;testing;fix},
  doi       = {10.1109/ICSE48619.2023.00156}
}

@misc{chen2025seedproverdeepbroadreasoning,
  title         = {Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving},
  author        = {Luoxin Chen and Jinming Gu and Liankai Huang and Wenhao Huang and Zhicheng Jiang and Allan Jie and Xiaoran Jin and Xing Jin and Chenggang Li and Kaijing Ma and Cheng Ren and Jiawei Shen and Wenlei Shi and Tong Sun and He Sun and Jiahui Wang and Siran Wang and Zhihong Wang and Chenrui Wei and Shufa Wei and Yonghui Wu and Yuchen Wu and Yihang Xia and Huajian Xin and Fan Yang and Huaiyuan Ying and Hongyi Yuan and Zheng Yuan and Tianyang Zhan and Chi Zhang and Yue Zhang and Ge Zhang and Tianyun Zhao and Jianqiu Zhao and Yichi Zhou and Thomas Hanwen Zhu},
  year          = {2025},
  eprint        = {2507.23726},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2507.23726}
}

@misc{xia2025livesweagentsoftwareengineeringagents,
  title         = {Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?},
  author        = {Chunqiu Steven Xia and Zhe Wang and Yan Yang and Yuxiang Wei and Lingming Zhang},
  year          = {2025},
  eprint        = {2511.13646},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE},
  url           = {https://arxiv.org/abs/2511.13646}
}

@article{brendel2025dilemma,
  author     = {Brendel, Ana and Sivaraman, Aishwarya and Millstein, Todd},
  title      = {Synthesizing Implication Lemmas for Interactive Theorem Proving},
  year       = {2025},
  issue_date = {October 2025},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {9},
  number     = {OOPSLA2},
  url        = {https://doi.org/10.1145/3763131},
  doi        = {10.1145/3763131},
  abstract   = {Interactive theorem provers (ITP) enable programmers to formally verify properties of their software systems. One burden for users of ITPs is identifying the necessary helper lemmas to complete a proof, for example those that define key inductive invariants. Existing approaches to lemma synthesis for ITPs have limited, if any, support for synthesizing implications: lemmas of the form P1 ∧ ⋯ ∧ Pn ⇒ Q. In this paper, we propose a technique and associated tool for synthesizing useful implication lemmas. Our approach employs a form of data-driven invariant inference to explore strengthenings of the current proof state, based on sample valuations of the current goal and assumptions. We have implemented our approach in a Rocq tactic called dilemma. We demonstrate its effectiveness in synthesizing necessary helper lemmas for proofs from the Verified Functional Algorithms textbook as well as from prior benchmark suites for lemma synthesis.},
  journal    = {Proc. ACM Program. Lang.},
  month      = oct,
  articleno  = {353},
  numpages   = {25},
  keywords   = {Data-Driven Synthesis, Interactive Theorem Prover, Lemma Synthesis}
}